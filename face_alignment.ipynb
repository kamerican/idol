{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "from skimage import io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as patches\n",
    "matplotlib.rcParams['figure.dpi'] = 300\n",
    "import cv2\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Import face_alignment last due to a weird OpenMP error\n",
    "import face_alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# reload(face_alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BASE_DIR = Path(__file__).parent.parent\n",
    "# BASE_DIR = Path.cwd().\n",
    "DRIVE_NAME = \"F:\\\\\"\n",
    "DATA_DIR = Path(DRIVE_NAME).joinpath('data')\n",
    "IMAGE_DIR = DATA_DIR.joinpath('images')\n",
    "DUMP_DIR = DATA_DIR.joinpath('dump')\n",
    "MODEL_DIR = DATA_DIR.joinpath('models')\n",
    "TEST_DIR = DATA_DIR.joinpath('test')\n",
    "# for c in DATA_DIR.iterdir(): print(c)\n",
    "# test_dir = IMAGE_DIR.joinpath('2020-08-17')\n",
    "# test_dir = IMAGE_DIR.joinpath('2020-06-25')\n",
    "# test_dir = IMAGE_DIR.joinpath('2012-06-11')\n",
    "\n",
    "class FaceDetection:\n",
    "    w_1: int\n",
    "    h_1: int\n",
    "    w_2: int\n",
    "    h_2: int\n",
    "        \n",
    "    side_length: int\n",
    "    confidence: float\n",
    "\n",
    "    def __init__(self, face_detection: list, multiplier: float):\n",
    "        w_1 = face_detection[0]\n",
    "        h_1 = face_detection[1]\n",
    "        w_2 = face_detection[2]\n",
    "        h_2 = face_detection[3]\n",
    "                \n",
    "        height = h_2 - h_1\n",
    "        width = w_2 - w_1\n",
    "        height_center = h_2 - (h_2 - h_1)/2\n",
    "        width_center = w_2 - (w_2 - w_1)/2\n",
    "        if width > height:\n",
    "            self.side_length = width * multiplier\n",
    "        else:\n",
    "            self.side_length = height * multiplier\n",
    "        self.confidence = face_detection[4]\n",
    "        self.h_1 = math.floor(height_center - self.side_length/2)\n",
    "        self.h_2 = math.ceil(height_center + self.side_length/2)\n",
    "        self.w_1 = math.floor(width_center - self.side_length/2)\n",
    "        self.w_2 = math.ceil(width_center + self.side_length/2)\n",
    "        \n",
    "@dataclass\n",
    "class Predtype:\n",
    "    list_slice: slice\n",
    "    color: tuple\n",
    "\n",
    "pred_types = {\n",
    "    'face': Predtype(slice(0, 17), (0.682, 0.780, 0.909, 0.5)),\n",
    "    'eyebrow1': Predtype(slice(17, 22), (1.0, 0.498, 0.055, 0.4)),\n",
    "    'eyebrow2': Predtype(slice(22, 27), (1.0, 0.498, 0.055, 0.4)),\n",
    "    'nose': Predtype(slice(27, 31), (0.345, 0.239, 0.443, 0.4)),\n",
    "    'nostril': Predtype(slice(31, 36), (0.345, 0.239, 0.443, 0.4)),\n",
    "    'eye1': Predtype(slice(36, 42), (0.596, 0.875, 0.541, 0.3)),\n",
    "    'eye2': Predtype(slice(42, 48), (0.596, 0.875, 0.541, 0.3)),\n",
    "    'lips': Predtype(slice(48, 60), (0.596, 0.875, 0.541, 0.3)),\n",
    "    'teeth': Predtype(slice(60, 68), (0.596, 0.875, 0.541, 0.4))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "facealignment_model = face_alignment.FaceAlignment(\n",
    "    face_alignment.LandmarksType._3D,\n",
    "    # face_alignment.LandmarksType._2D,\n",
    "    face_detector='sfd',\n",
    "    # face_detector='blazeface',\n",
    "    flip_input=False,\n",
    "    device='cpu',\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vggface_model = VGGFace(\n",
    "    # model='resnet50',\n",
    "    model='senet50',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling='avg',\n",
    ")\n",
    "print('Inputs: %s' % vggface_model.inputs)\n",
    "print('Outputs: %s' % vggface_model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_detection(detection: list, multiplication_factor: float) -> list:\n",
    "    if len(detection) != 5:\n",
    "        print(\"Can't rescale a detection that has more than 4+1 items!\")\n",
    "        return detection\n",
    "    detection[0] = detection[0]/multiplication_factor\n",
    "    detection[1] = detection[1]/multiplication_factor\n",
    "    detection[2] = detection[2]/multiplication_factor\n",
    "    detection[3] = detection[3]/multiplication_factor\n",
    "    return detection\n",
    "def rescale_landmark(landmark: list, multiplication_factor: float) -> list:\n",
    "    return np.array(landmark)/multiplication_factor\n",
    "def get_landmarks_and_detection(facealignment_model: face_alignment.FaceAlignment, img: np.ndarray, sfd_resize_p=480):\n",
    "    # standard format is (WIDTH x HEIGHT)\n",
    "    # but img.shape returns it in opposite order\n",
    "    height, width, _ = img.shape\n",
    "    if height > width:\n",
    "        # Vertical image\n",
    "        multiplication_factor = sfd_resize_p/width\n",
    "        dim = (sfd_resize_p, int(height*multiplication_factor))\n",
    "    else:\n",
    "        # Horizontal image\n",
    "        multiplication_factor = sfd_resize_p/height\n",
    "        dim = (int(width*multiplication_factor), sfd_resize_p)\n",
    "    resized_img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
    "    # Get lists of landmarks and detections, which are both lists of coordinates\n",
    "    landmarks, detections = facealignment_model.get_landmarks_from_image(resized_img)\n",
    "    if landmarks is None and detections is None:\n",
    "        return None, None\n",
    "    rescaled_landmarks = [rescale_landmark(landmark, multiplication_factor) for landmark in landmarks]\n",
    "    rescaled_detections = [rescale_detection(detection, multiplication_factor) for detection in detections]    \n",
    "    return rescaled_landmarks, rescaled_detections\n",
    "def get_embeddings(vggface_model: VGGFace, img: np.ndarray, detections: list, vgg_resize_p=224, show_input_face=False):\n",
    "    embeddings = []\n",
    "    for index, detection in enumerate(detections):\n",
    "        detection_obj = FaceDetection(detection, multiplier=1.2)\n",
    "        print(\"- Confidence: {}\".format(round(detection_obj.confidence, 3)))\n",
    "        face_img = img[\n",
    "            detection_obj.h_1:detection_obj.h_2,\n",
    "            detection_obj.w_1:detection_obj.w_2,\n",
    "        ]\n",
    "\n",
    "        if detection_obj.side_length > vgg_resize_p:\n",
    "            interp_method = cv2.INTER_AREA\n",
    "        else:\n",
    "            interp_method = cv2.INTER_CUBIC\n",
    "        face_img = cv2.resize(face_img, (vgg_resize_p, vgg_resize_p), interpolation=interp_method)\n",
    "\n",
    "        if show_input_face:\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_axes([0, 0, 1, 1])\n",
    "            ax.imshow(face_img)\n",
    "            ax.axis('off')\n",
    "\n",
    "        samples = preprocess_input(np.expand_dims(face_img.astype('float32'), axis=0), version=2)\n",
    "        embeddings.append(vggface_model.predict(samples))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_as_list_of_dicts = []\n",
    "for image_dir in TEST_DIR.iterdir():\n",
    "    print(\"----- Processing images in {}\".format(image_dir))\n",
    "    files = list(image_dir.iterdir())\n",
    "    f_i = 0\n",
    "    f_n = len(files)\n",
    "    for f in files:\n",
    "        # somehow check if directory has already been processed\n",
    "        if(False):\n",
    "            print(\"!!!!! Directory already processed\")\n",
    "            break\n",
    "        if f.suffix not in ['.jpg', '.png', '.jfif']:\n",
    "            print(\"!!!!! Not an image: {}\".format(f))\n",
    "        else:\n",
    "            f_i += 1\n",
    "            print(\"--- Processing {}/{}, {}\".format(\n",
    "                f_i,\n",
    "                f_n,\n",
    "                f.name\n",
    "            ))\n",
    "            img = io.imread(f)\n",
    "            landmarks, detections = get_landmarks_and_detection(facealignment_model, img)\n",
    "            if landmarks is None and detections is None:\n",
    "                print(\"--- 0 faces detected, continuing to next image\")\n",
    "                continue\n",
    "            print(\"--- {} faces detected\".format(len(detections)))\n",
    "            embeddings = get_embeddings(vggface_model, img, detections)\n",
    "            for index, embedding in enumerate(embeddings):\n",
    "                df_as_list_of_dicts.append({\n",
    "                    'img_path': str(f),\n",
    "                    'detection': detections[index],\n",
    "                    'landmarks': landmarks[index],\n",
    "                    'embedding': embedding\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame.from_dict(df_as_list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(type(row['embedding'][0]))\n",
    "    # print(row['embedding'][0].shape)\n",
    "# type(column_as_list_of_dicts[0]['embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(DUMP_DIR / \"test_dataframe.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_pickle(DUMP_DIR / \"test_dataframe.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_path = None\n",
    "plot_style = dict(\n",
    "    marker='.',\n",
    "    markersize=1,\n",
    "    linestyle='none',\n",
    "    # linewidth=2,\n",
    ")\n",
    "\n",
    "# Analyze images in dataframe\n",
    "for index, row in df.iterrows():\n",
    "    detection = FaceDetection(row['detection'], multiplier=1.25)\n",
    "\n",
    "\n",
    "    # Draw image if looking at a different image (or first image) than the last image\n",
    "    if img_path is None or img_path != row['img_path']:\n",
    "        img_path = row['img_path']\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "        ax.imshow(io.imread(img_path))\n",
    "        ax.axis('off')\n",
    "        print(\"Confidence: {}\".format(detection.confidence))\n",
    "\n",
    "    # Draw bounding box\n",
    "    rect = patches.Rectangle(\n",
    "        (detection.w_1, detection.h_1),\n",
    "        detection.side_length,\n",
    "        detection.side_length,\n",
    "        linewidth=1,\n",
    "        edgecolor='g',\n",
    "        facecolor='none',\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    plt.plot(detection.w_1, detection.h_1, 'ro')\n",
    "    plt.plot(detection.w_1, detection.h_2, 'go')\n",
    "    plt.plot(detection.w_2, detection.h_1, 'bo')\n",
    "    plt.plot(detection.w_2, detection.h_2, 'yo')\n",
    "\n",
    "\n",
    "\n",
    "    # Draw 2D landmarks\n",
    "\n",
    "# for img_path, faces in img_preds.items():\n",
    "#     # print(img_path)\n",
    "#     # print(len(value))\n",
    "#     # print(\"\\n\")\n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_axes([0, 0, 1, 1])\n",
    "#     ax.imshow(io.imread(img_path))\n",
    "    # ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "    # for index, landmark in enumerate(row['landmarks']):\n",
    "    # landmark = \n",
    "    # print(landmark)\n",
    "    # ax = fig.add_subplot(1, len(faces), index + 1)\n",
    "    for pred_type in pred_types.values():\n",
    "        # print(pred)\n",
    "        # print(type(pred))\n",
    "        \n",
    "        ax.plot(\n",
    "            row['landmarks'][pred_type.list_slice, 0],\n",
    "            row['landmarks'][pred_type.list_slice, 1],\n",
    "            color=pred_type.color,\n",
    "            **plot_style,\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    img = io.imread(row['img_path'])\n",
    "    detection = FaceDetection(row['detection'], multiplier=1.2)\n",
    "\n",
    "\n",
    "    # add code to save stuff\n",
    "    # add code from extra_code to check bbox off image edges\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python (idol_env)",
   "language": "python",
   "name": "idol_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}